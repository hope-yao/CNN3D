{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n"
     ]
    }
   ],
   "source": [
    "print(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 2 3 5 8 13 21 34 55\n"
     ]
    }
   ],
   "source": [
    "# generator version\n",
    "def fibon(n):\n",
    "    a = b = 1\n",
    "    for i in xrange(n):\n",
    "        yield a\n",
    "        a, b = b, a + b\n",
    "\n",
    "for x in fibon(10):\n",
    "    print x,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "''' generator, ref: http://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do'''\n",
    "\n",
    "filter_sizes = [1,2]\n",
    "feature_maps = [3,4]\n",
    "conv_parameters = zip(filter_sizes, feature_maps)\n",
    "c = (a+b for i, (a, b) in enumerate(conv_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init D\n",
      "B.__init__()\n",
      "C.__init__()\n",
      "A.__init__()\n"
     ]
    }
   ],
   "source": [
    "'''class hierarchy, super'''\n",
    "class A(object):\n",
    "    def __init__(self,**kwargs):\n",
    "        print(\"A.__init__()\")\n",
    "#         print(kwargs['c'])\n",
    "\n",
    "class B(A):\n",
    "    def __init__(self,**kwargs):\n",
    "        print(\"B.__init__()\")\n",
    "        for name, value in kwargs.items():\n",
    "            print(value)\n",
    "        super(B, self).__init__(**kwargs)\n",
    "\n",
    "class C(A):\n",
    "    def __init__(self,**kwargs):\n",
    "        print(\"C.__init__()\")\n",
    "        for name, value in kwargs.items():\n",
    "            print(value)\n",
    "        super(C,self).__init__(**kwargs)\n",
    "        \n",
    "class D(B,C):\n",
    "    def __init__(self,a,**kwargs):\n",
    "        print(\"init D\")\n",
    "        super(D,self).__init__(**kwargs)\n",
    "d = D(a=0,b=1,c=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entering func1\n",
      "inside func1()\n",
      "Exited func1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'new_f'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''decorator'''\n",
    "def entryExit(f):\n",
    "    def new_f():\n",
    "        print \"Entering\", f.__name__\n",
    "        f()\n",
    "        print \"Exited\", f.__name__\n",
    "    return new_f\n",
    "\n",
    "@entryExit\n",
    "def func1():\n",
    "    print \"inside func1()\"\n",
    "\n",
    "@entryExit\n",
    "def func2():\n",
    "    print \"inside func2()\"\n",
    "func1()\n",
    "func2.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start magic\n",
      "normal call\n",
      "end magic\n"
     ]
    }
   ],
   "source": [
    "'''decorator inside a class for class method'''\n",
    "class Test(object):\n",
    "    def _decorator(foo):\n",
    "        def magic( self ) :\n",
    "            print \"start magic\"\n",
    "            foo( self )\n",
    "            print \"end magic\"\n",
    "        return magic\n",
    "\n",
    "    @_decorator\n",
    "    def bar( self ) :\n",
    "        print \"normal call\"\n",
    "\n",
    "test = Test()\n",
    "\n",
    "test.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in decorator before wrapee with flag  foo de fa fa\n",
      "in bar x y z\n",
      "in decorator after wrapee with flag  foo de fa fa\n"
     ]
    }
   ],
   "source": [
    "'''decorator outside class'''\n",
    "class MyDec(object):\n",
    "    def __init__(self,flag):\n",
    "        self.flag = flag\n",
    "    def __call__(self, original_func):\n",
    "        decorator_self = self\n",
    "        def wrappee( *args, **kwargs):\n",
    "            print 'in decorator before wrapee with flag ',decorator_self.flag\n",
    "            original_func(*args,**kwargs)\n",
    "            print 'in decorator after wrapee with flag ',decorator_self.flag\n",
    "        return wrappee\n",
    "\n",
    "@MyDec('foo de fa fa')\n",
    "def bar(a,b,c):\n",
    "    print 'in bar',a,b,c\n",
    "bar('x','y','z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>John Doe</p>\n"
     ]
    }
   ],
   "source": [
    "'''decorator inside a class for a function'''\n",
    "def p_decorate(func):\n",
    "    def func_wrapper(self):\n",
    "        return \"<p>{0}</p>\".format(func(self))\n",
    "    return func_wrapper\n",
    "\n",
    "class Person(object):\n",
    "    def __init__(self):\n",
    "        self.name = \"John\"\n",
    "        self.family = \"Doe\"\n",
    "\n",
    "    @p_decorate\n",
    "    def get_fullname(self):\n",
    "        return self.name+\" \"+self.family\n",
    "\n",
    "my_person = Person()\n",
    "print my_person.get_fullname()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "float32\n",
      "(4, 2, 3, 3)\n",
      "[[[[  18.   18.   18.]\n",
      "   [  18.   18.   18.]\n",
      "   [  18.   18.   18.]]\n",
      "\n",
      "  [[   0.    0.    0.]\n",
      "   [   0.    0.    0.]\n",
      "   [   0.    0.    0.]]]\n",
      "\n",
      "\n",
      " [[[   0.    0.    0.]\n",
      "   [   0.    0.    0.]\n",
      "   [   0.    0.    0.]]\n",
      "\n",
      "  [[ 108.  108.  108.]\n",
      "   [ 108.  108.  108.]\n",
      "   [ 108.  108.  108.]]]\n",
      "\n",
      "\n",
      " [[[   0.    0.    0.]\n",
      "   [   0.    0.    0.]\n",
      "   [   0.    0.    0.]]\n",
      "\n",
      "  [[   0.    0.    0.]\n",
      "   [   0.    0.    0.]\n",
      "   [   0.    0.    0.]]]\n",
      "\n",
      "\n",
      " [[[   0.    0.    0.]\n",
      "   [   0.    0.    0.]\n",
      "   [   0.    0.    0.]]\n",
      "\n",
      "  [[   0.    0.    0.]\n",
      "   [   0.    0.    0.]\n",
      "   [   0.    0.    0.]]]]\n"
     ]
    }
   ],
   "source": [
    "'''conv3d in theano and cudnn'''\n",
    "\n",
    "%reset\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.cuda.dnn import dnn_conv, dnn_conv3d\n",
    "from theano.tensor.nnet import conv2d\n",
    "import numpy as np\n",
    "\n",
    "theano.config.floatX = 'float32'\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "print(theano.config.floatX)\n",
    "\n",
    "conv_dim = 2\n",
    "if conv_dim==2:\n",
    "    dtensor4 = T.TensorType(floatX, (False,)*4)\n",
    "    img = dtensor4(name='input')\n",
    "    kerns = dtensor4(name='weight')\n",
    "    base = dnn_conv(img = img, kerns = kerns, border_mode = 'valid', conv_mode = 'conv')        \n",
    "#     conved = base.sum(1)\n",
    "    conved = base\n",
    "    myconv2d = theano.function([img,kerns], conved)\n",
    "    \n",
    "    a = np.zeros((4,3,5,5), dtype=floatX)\n",
    "    a[0,0,:,:] = np.ones((5,5), dtype=floatX)\n",
    "    a[1,1,:,:] = 3*np.ones((5,5), dtype=floatX)\n",
    "    b = np.zeros((2,3,3,3), dtype=floatX)\n",
    "    b[0,0,:,:] = 2*np.ones((3,3), dtype=floatX)\n",
    "    b[1,1,:,:] = 4*np.ones((3,3), dtype=floatX)\n",
    "\n",
    "    c = np.asarray(myconv2d(a,b))\n",
    "    print(c.shape)\n",
    "    print(c)\n",
    "    \n",
    "else:        \n",
    "    dtensor5 = T.TensorType(floatX, (False,)*5)\n",
    "    img = dtensor5(name='input')\n",
    "    kerns = dtensor5(name='weight')\n",
    "#     dtensor4 = T.TensorType(floatX, (False,)*4)\n",
    "#     subsample = dtensor4(name='weight')\n",
    "    base = dnn_conv3d(img = img, kerns = kerns, border_mode = 'valid', conv_mode = 'conv')     \n",
    "#     conved = base.sum(1)\n",
    "    conved = base\n",
    "    conv3d = theano.function([img,kerns], conved)\n",
    "\n",
    "    a = np.zeros((4,3,5,5,5), dtype=floatX)\n",
    "    b = np.zeros((4,3,3,3,3), dtype=floatX) # # of channel in next layer, # of channel this layer, volume size\n",
    "    print(a.dtype)\n",
    "    print('input size',a.shape)\n",
    "    print('filter size',b.shape)\n",
    "    c = np.asarray(conv3d(a,b))\n",
    "    print('output size',c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "(4, 2, 3, 3)\n",
      "[[[[ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]]]\n",
      "\n",
      "\n",
      " [[[ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]]\n",
      "\n",
      "  [[ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]\n",
      "   [ 0.  0.  0.]]]]\n"
     ]
    }
   ],
   "source": [
    "'''2d cnn layer in blocks'''\n",
    "%reset\n",
    "import numpy as np\n",
    "import theano\n",
    "from theano import tensor\n",
    "from blocks.bricks.base import Brick\n",
    "from blocks.bricks.conv import (Convolutional, ConvolutionalSequence,Flattener, MaxPooling)\n",
    "from toolz.itertoolz import interleave\n",
    "from blocks.initialization import Constant, Uniform, IsotropicGaussian\n",
    "from blocks.bricks import MLP, Tanh, Softmax\n",
    "theano.config.floatX = 'float32'\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "convnet = Convolutional(\n",
    "                    filter_size=(3,3),\n",
    "                    num_filters=2,\n",
    "                    num_channels=3,\n",
    "                    step=(1,1),\n",
    "                    border_mode='valid',\n",
    "                    weights_init = Constant(0),\n",
    "                    biases_init=Constant(0),\n",
    "                    name='conv_0')\n",
    "convnet.initialize()\n",
    "x = tensor.tensor4('features')\n",
    "h = convnet.apply(x)\n",
    "f = theano.function([x], h)\n",
    "\n",
    "a = np.zeros((4,3,5,5), dtype=floatX)\n",
    "a[0,0,:,:] = np.ones((5,5), dtype=floatX)\n",
    "a[1,1,:,:] = 3*np.ones((5,5), dtype=floatX)\n",
    "# b = np.zeros((4,3,3,3), dtype=floatX)\n",
    "# b[0,0,:,:] = 2*np.ones((3,3), dtype=floatX)\n",
    "# b[1,1,:,:] = 4*np.ones((3,3), dtype=floatX)\n",
    "c = f(a)\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "float32\n",
      "input size (2, 3, 5, 5, 5)\n",
      "filter size (2, 3, 3, 3, 3)\n",
      "[[[[[  54.   54.   54.]\n",
      "    [  54.   54.   54.]\n",
      "    [  54.   54.   54.]]\n",
      "\n",
      "   [[  54.   54.   54.]\n",
      "    [  54.   54.   54.]\n",
      "    [  54.   54.   54.]]\n",
      "\n",
      "   [[  54.   54.   54.]\n",
      "    [  54.   54.   54.]\n",
      "    [  54.   54.   54.]]]\n",
      "\n",
      "\n",
      "  [[[   0.    0.    0.]\n",
      "    [   0.    0.    0.]\n",
      "    [   0.    0.    0.]]\n",
      "\n",
      "   [[   0.    0.    0.]\n",
      "    [   0.    0.    0.]\n",
      "    [   0.    0.    0.]]\n",
      "\n",
      "   [[   0.    0.    0.]\n",
      "    [   0.    0.    0.]\n",
      "    [   0.    0.    0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[   0.    0.    0.]\n",
      "    [   0.    0.    0.]\n",
      "    [   0.    0.    0.]]\n",
      "\n",
      "   [[   0.    0.    0.]\n",
      "    [   0.    0.    0.]\n",
      "    [   0.    0.    0.]]\n",
      "\n",
      "   [[   0.    0.    0.]\n",
      "    [   0.    0.    0.]\n",
      "    [   0.    0.    0.]]]\n",
      "\n",
      "\n",
      "  [[[ 324.  324.  324.]\n",
      "    [ 324.  324.  324.]\n",
      "    [ 324.  324.  324.]]\n",
      "\n",
      "   [[ 324.  324.  324.]\n",
      "    [ 324.  324.  324.]\n",
      "    [ 324.  324.  324.]]\n",
      "\n",
      "   [[ 324.  324.  324.]\n",
      "    [ 324.  324.  324.]\n",
      "    [ 324.  324.  324.]]]]]\n"
     ]
    }
   ],
   "source": [
    "% reset\n",
    "'''native 3d cnn brick in blocks'''\n",
    "import numpy as np\n",
    "from blocks.bricks.base import application, lazy\n",
    "from blocks.bricks import Brick\n",
    "from blocks.bricks import Feedforward, Initializable\n",
    "from theano.sandbox.cuda.dnn import dnn_conv, dnn_conv3d\n",
    "from theano.tensor.nnet import conv2d\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "theano.config.floatX = 'float32'\n",
    "floatX = theano.config.floatX\n",
    "\n",
    "class Conv3d(Feedforward,Initializable):\n",
    "#     def __init__(**args):\n",
    "#         kerns\n",
    "    @application(inputs=['input1', 'input2'], outputs=['output'])\n",
    "    def apply(self, input1, input2):\n",
    "        base = dnn_conv3d(img = input1, kerns = input2, border_mode = 'valid', conv_mode = 'conv')     \n",
    "        conved = base\n",
    "        return conved\n",
    "conv3d = Conv3d()\n",
    "conv3d.initialize()\n",
    "\n",
    "# symbolic graph\n",
    "dtensor5 = T.TensorType(floatX, (False,)*5)\n",
    "img = dtensor5(name='input')\n",
    "kerns = dtensor5(name='weight')\n",
    "h = conv3d.apply(img,kerns)\n",
    "f = theano.function([img,kerns], h)\n",
    "\n",
    "# assign number\n",
    "a = np.zeros((2,3,5,5,5), dtype=floatX)\n",
    "b = np.zeros((2,3,3,3,3), dtype=floatX) # # of channel in next layer, # of channel this layer, volume size\n",
    "a[0,0,:,:,:] = np.ones((5,5), dtype=floatX)\n",
    "a[1,1,:,:,:] = 3*np.ones((5,5,5), dtype=floatX)\n",
    "b[0,0,:,:,:] = 2*np.ones((3,3,3), dtype=floatX)\n",
    "b[1,1,:,:,:] = 4*np.ones((3,3,3), dtype=floatX)\n",
    "print(a.dtype)\n",
    "print('input size',a.shape)\n",
    "print('filter size',b.shape)\n",
    "c = conv3d.apply(a,b)\n",
    "print(c.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "% reset\n",
    "from blocks.bricks.base import application, lazy\n",
    "from blocks.bricks import Feedforward, Linear\n",
    "from blocks.initialization import Constant, Uniform, IsotropicGaussian\n",
    "\n",
    "class ChainOfTwoFeedforward(Feedforward):\n",
    "    '''Two sequential Feedforward bricks'''\n",
    "    def __init__(self, brick1, brick2, **kwargs):\n",
    "        self.brick1 = brick1\n",
    "        self.brick2 = brick2\n",
    "        children = [self.brick1, self.brick2]\n",
    "        kwargs.setdefault('children', []).extend(children)\n",
    "        super(Feedforward, self).__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def input_dim(self):\n",
    "        return self.brick1.input_dim\n",
    "\n",
    "    @input_dim.setter\n",
    "    def input_dim(self, value):\n",
    "        self.brick1.input_dim = value\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self.brick2.output_dim\n",
    "\n",
    "    @output_dim.setter\n",
    "    def output_dim(self, value):\n",
    "        self.brick2.output_dim = value\n",
    "\n",
    "    def _push_allocation_config(self):\n",
    "        self.brick2.input_dim = self.brick1.get_dim('output')\n",
    "\n",
    "    @application\n",
    "    def apply(self, x):\n",
    "        return self.brick2.apply(self.brick1.apply(x))\n",
    "    \n",
    "brick1 = Linear(input_dim=3, output_dim=2, use_bias=False,weights_init=Constant(2))\n",
    "brick2 = Linear(output_dim=4, use_bias=False, weights_init=Constant(2))\n",
    "seq = ChainOfTwoFeedforward(brick1, brick2)\n",
    "seq.initialize()\n",
    "brick2.input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "(Shape.0, Shape.0)\n",
      "(Shape.0, Shape.0)\n",
      "(Shape.0, Shape.0)\n",
      "(Shape.0, Shape.0)\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "from blocks.bricks.sequences import MLP\n",
    "from blocks.bricks import Initializable, Tanh, Softmax, Identity\n",
    "from blocks.initialization import IsotropicGaussian, Constant\n",
    "\n",
    "# mlp = MLP(activations=[Tanh(), Softmax()], dims=[784, 100, 10],weights_init=IsotropicGaussian(0.01), biases_init=Constant(0))\n",
    "# mlp.initialize()\n",
    "# x = tensor.matrix('features')\n",
    "# y = tensor.lmatrix('targets')\n",
    "# y_hat = mlp.apply(x)\n",
    "\n",
    "import theano.tensor as tensor\n",
    "from blocks.bricks import Rectifier\n",
    "dim_att = 5\n",
    "n0 = 3\n",
    "l = tensor.matrix('l') # for a batch\n",
    "inits = {\n",
    "    'weights_init': IsotropicGaussian(0.01),\n",
    "    'biases_init': Constant(0.),\n",
    "}\n",
    "# rho = tensor.lmatrix('targets')\n",
    "x = tensor.matrix('features')  # keyword from fuel\n",
    "from attention_module.attention import ZoomableAttentionWindow\n",
    "zoomer = ZoomableAttentionWindow(1, 28, 28, 5)\n",
    "rho = zoomer.read_large(x, l[1], l[0]) # glimpse sensor in 2D\n",
    "linear0 = MLP(activations=[Identity()], dims=[dim_att*dim_att, n0], name=\"glimpse network 0\", **inits)\n",
    "rect0 = Rectifier()\n",
    "h_g = rect0.apply(linear0.apply(rho))  # theta_g^1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.  1.  1.  1.]\n",
      "  [ 1.  1.  1.  1.]]\n",
      "\n",
      " [[ 2.  2.  2.  2.]\n",
      "  [ 2.  2.  2.  2.]]\n",
      "\n",
      " [[ 3.  3.  3.  3.]\n",
      "  [ 3.  3.  3.  3.]]\n",
      "\n",
      " [[ 4.  4.  4.  4.]\n",
      "  [ 4.  4.  4.  4.]]\n",
      "\n",
      " [[ 5.  5.  5.  5.]\n",
      "  [ 5.  5.  5.  5.]]]\n"
     ]
    }
   ],
   "source": [
    "'''simple RNN in blocks'''\n",
    "import numpy\n",
    "import theano\n",
    "from theano import tensor\n",
    "from blocks import initialization\n",
    "from blocks.bricks import Identity\n",
    "from blocks.bricks.recurrent import SimpleRecurrent\n",
    "\n",
    "x = tensor.tensor3('x')\n",
    "rnn = SimpleRecurrent(dim=4, activation=Identity(), weights_init=initialization.Identity())\n",
    "rnn.initialize()\n",
    "h = rnn.apply(x)\n",
    "f = theano.function([x], h)\n",
    "print(f(numpy.ones((5, 2, 4), dtype=theano.config.floatX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[  1.   1.   1.]\n",
      "  [  1.   1.   1.]]\n",
      "\n",
      " [[  3.   3.   3.]\n",
      "  [  3.   3.   3.]]\n",
      "\n",
      " [[  8.   8.   8.]\n",
      "  [  8.   8.   8.]]\n",
      "\n",
      " [[ 21.  21.  21.]\n",
      "  [ 21.  21.  21.]]]\n",
      "[[[  1.   1.   1.]\n",
      "  [  1.   1.   1.]]\n",
      "\n",
      " [[  4.   4.   4.]\n",
      "  [  4.   4.   4.]]\n",
      "\n",
      " [[ 12.  12.  12.]\n",
      "  [ 12.  12.  12.]]\n",
      "\n",
      " [[ 33.  33.  33.]\n",
      "  [ 33.  33.  33.]]]\n"
     ]
    }
   ],
   "source": [
    "'''two layer rnn in blocks'''\n",
    "import numpy\n",
    "import theano\n",
    "from theano import tensor\n",
    "from blocks.bricks import Identity\n",
    "from blocks import initialization\n",
    "from blocks.bricks.recurrent import BaseRecurrent, recurrent\n",
    "from blocks.bricks.recurrent import SimpleRecurrent\n",
    "class FeedbackRNN(BaseRecurrent):\n",
    "    def __init__(self, dim, **kwargs):\n",
    "        super(FeedbackRNN, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.first_recurrent_layer = SimpleRecurrent(dim=self.dim, activation=Identity(), name='first_recurrent_layer',weights_init=initialization.Identity())\n",
    "        self.second_recurrent_layer = SimpleRecurrent(dim=self.dim, activation=Identity(), name='second_recurrent_layer',weights_init=initialization.Identity())\n",
    "        self.children = [self.first_recurrent_layer,self.second_recurrent_layer]\n",
    "\n",
    "    @recurrent(sequences=['inputs'], contexts=[],states=['first_states', 'second_states'],outputs=['first_states', 'second_states'])\n",
    "    def apply(self, inputs, first_states=None, second_states=None):\n",
    "        first_h = self.first_recurrent_layer.apply(inputs=inputs, states=first_states + second_states, iterate=False)\n",
    "        second_h = self.second_recurrent_layer.apply(inputs=first_h, states=second_states, iterate=False)\n",
    "        return first_h, second_h\n",
    "\n",
    "    def get_dim(self, name):\n",
    "        return (self.dim if name in ('inputs', 'first_states', 'second_states')\n",
    "        else super(FeedbackRNN, self).get_dim(name))\n",
    "\n",
    "x = tensor.tensor3('x')\n",
    "feedback = FeedbackRNN(dim=3)\n",
    "feedback.initialize()\n",
    "first_h, second_h = feedback.apply(inputs=x)\n",
    "f = theano.function([x], [first_h, second_h])\n",
    "for states in f(numpy.ones((4, 2, 3), dtype=theano.config.floatX)):\n",
    "    print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''save data to hdf5 format, from Max'''\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tarfile, os\n",
    "import sys\n",
    "import cStringIO as StringIO\n",
    "import tarfile\n",
    "import time\n",
    "import zlib\n",
    "\n",
    "PREFIX = 'data/'\n",
    "SUFFIX = '.npy.z'\n",
    "\n",
    "class NpyTarReader(object):\n",
    "    def __init__(self, fname):\n",
    "        self.tfile = tarfile.open(fname, 'r|')\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def next(self):\n",
    "        entry = self.tfile.next()\n",
    "        if entry is None:\n",
    "            raise StopIteration()\n",
    "        name = entry.name[len(PREFIX):-len(SUFFIX)]\n",
    "        fileobj = self.tfile.extractfile(entry)\n",
    "        buf = zlib.decompress(fileobj.read())\n",
    "        arr = np.load(StringIO.StringIO(buf))\n",
    "        return arr, name\n",
    "\n",
    "    def close(self):\n",
    "        self.tfile.close()\n",
    "\n",
    "train_dataset = NpyTarReader('/home/hope-yao/Documents/Data/pot_cup/more_data_mixsal0.1/shapenet10_train.tar')\n",
    "test_dataset = NpyTarReader('/home/hope-yao/Documents/Data/pot_cup/more_data_mixsal0.1/shapenet10_test.tar')\n",
    "\n",
    "train_features = []\n",
    "train_targets = []\n",
    "test_features = []\n",
    "test_targets = []\n",
    "for index, (array, name) in enumerate(train_dataset):\n",
    "    if int(name[-3:])==1:\n",
    "#         train_features.append(array.flatten())\n",
    "        train_features.append(array.reshape(1,array.shape[0],array.shape[1],array.shape[2]))\n",
    "        train_targets.append([int(name[0:3])])\n",
    "for index, (array, name) in enumerate(test_dataset):\n",
    "    if int(name[-3:]) == 1:\n",
    "#         test_features.append(array.flatten())\n",
    "        test_features.append(array.reshape(1,array.shape[0],array.shape[1],array.shape[2]))\n",
    "        test_targets.append([int(name[0:3])])\n",
    "\n",
    "train_features = np.array(train_features)\n",
    "train_targets = np.array(train_targets)-1 #starts from 0\n",
    "test_features = np.array(test_features)\n",
    "test_targets = np.array(test_targets)-1\n",
    "train_n, c, p1, p2, p3 = train_features.shape\n",
    "test_n = test_features.shape[0]\n",
    "n = train_n + test_n\n",
    "\n",
    "f = h5py.File('potcup_sal.hdf5', mode='w')\n",
    "features = f.create_dataset('input', (n, c, p1, p2, p3), dtype='uint8')\n",
    "targets = f.create_dataset('targets', (n, 1), dtype='uint8')\n",
    "\n",
    "features[...] = np.vstack([train_features, test_features])\n",
    "targets[...] = np.vstack([train_targets, test_targets])\n",
    "\n",
    "features.dims[0].label = 'batch'\n",
    "features.dims[1].label = 'input'\n",
    "targets.dims[0].label = 'batch'\n",
    "targets.dims[1].label = 'index'\n",
    "\n",
    "from fuel.datasets.hdf5 import H5PYDataset\n",
    "split_dict = {\n",
    "    'train': {'input': (0, train_n), 'targets': (0, train_n)},\n",
    "    'test': {'input': (train_n, n), 'targets': (train_n, n)}}\n",
    "f.attrs['split'] = H5PYDataset.create_split_array(split_dict)\n",
    "\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3977, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "'''loading fuel dataset'''\n",
    "% reset\n",
    "from fuel.datasets.hdf5 import H5PYDataset\n",
    "mnist_train = H5PYDataset('/home/hope-yao/Documents/Data/mnist.hdf5', which_sets=('train',))\n",
    "mnist_test = H5PYDataset('/home/hope-yao/Documents/Data/mnist.hdf5', which_sets=('test',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'features', u'targets')\n",
      "(60000, 1)\n",
      "(10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train.provides_sources)\n",
    "handle = mnist_train.open()\n",
    "train_data = mnist_train.get_data(handle, slice(0, 60000))\n",
    "handletest = mnist_test.open()\n",
    "test_data = mnist_test.get_data(handletest, slice(0, 10000))\n",
    "\n",
    "print((train_data[1].shape))\n",
    "train_features = train_data[0]\n",
    "train_targets = train_data[1]\n",
    "print((test_data[0].shape))\n",
    "test_features = test_data[0]\n",
    "test_targets = test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import tarfile, os\n",
    "import sys\n",
    "import cStringIO as StringIO\n",
    "import tarfile\n",
    "import time\n",
    "import zlib\n",
    "n = 70000\n",
    "c = 1\n",
    "p1 = p2 = 28\n",
    "train_n = 60000\n",
    "test_n = 10000\n",
    "\n",
    "f = h5py.File('bmnist.hdf5', mode='w')\n",
    "features = f.create_dataset('features', (n, c, p1, p2), dtype='uint8')\n",
    "targets = f.create_dataset('targets', (n, 1), dtype='uint8')\n",
    "\n",
    "train_features = train_features/128\n",
    "test_features = test_features/128\n",
    "\n",
    "features[...] = np.vstack([train_features, test_features])\n",
    "targets[...] = np.vstack([train_targets, test_targets])\n",
    "\n",
    "features.dims[0].label = 'batch'\n",
    "features.dims[1].label = 'input'\n",
    "targets.dims[0].label = 'batch'\n",
    "targets.dims[1].label = 'index'\n",
    "\n",
    "from fuel.datasets.hdf5 import H5PYDataset\n",
    "split_dict = {\n",
    "    'train': {'features': (0, train_n), 'targets': (0, train_n)},\n",
    "    'test': {'features': (train_n, n), 'targets': (train_n, n)}}\n",
    "f.attrs['split'] = H5PYDataset.create_split_array(split_dict)\n",
    "\n",
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import ShuffledScheme\n",
    "batch_size = 100\n",
    "train_set_stream = DataStream.default_stream(\n",
    "    train_set, iteration_scheme=ShuffledScheme(\n",
    "        train_set.num_examples, batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((False, False, False, True), (False,))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3200, 3, 32, 3200)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import theano.tensor as tensor\n",
    "A = tensor.zeros((3200, 3, 32, 1))\n",
    "B = tensor.zeros((3200))\n",
    "print(A.broadcastable,B.broadcastable)\n",
    "# B = B.transpose([0, 2, 1])\n",
    "# A.dimshuffle([0,1,2,'x']) * B.dimshuffle([0,'x',1,2])     \n",
    "C=(A*B).eval()\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''upsample in post-process'''\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "inarr = np.ones((32,32,32))\n",
    "def upsample(X,d):\n",
    "    XX = np.zeros((X.shape[0],X.shape[1],d*X.shape[2],d*X.shape[3],d*X.shape[4]))\n",
    "    for i,xi in enumerate(X):\n",
    "        for j,xij in enumerate(xi):\n",
    "            tmp = (ndimage.zoom(xij, d))\n",
    "            XX[i,j,:,:,:] = tmp\n",
    "    return np.float32(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''project upper layer filter back to the input layer'''\n",
    "from theano.sandbox.cuda.dnn import dnn_conv3d\n",
    "def deconv(W0,W1):\n",
    "    print(W0.shape)\n",
    "    print(W1.shape)\n",
    "    filter_size = W0.shape[2:5]\n",
    "    num_filter = W0.shape[0]\n",
    "    num_ch = W1.shape[0]\n",
    "    W1_layer0 = dnn_conv3d(img=W1,kerns=W0.transpose(1,0,2,3,4),border_mode='valid',subsample=(1,1,1))\n",
    "    W1_layer0 = np.asarray(W1_layer0.eval())\n",
    "    print(W1_layer0.shape)\n",
    "    return W1_layer0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (1,2,3)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''visualize 3D data'''\n",
    "def plot_cube(ax,x,y,z,a):\n",
    "    \"x y z location and alpha\"\n",
    "    inc = 1\n",
    "    ax.plot_surface([[x,x+inc],[x,x+inc]],[[y,y],[y+inc,y+inc]],z, alpha=a)\n",
    "    ax.plot_surface([[x,x+inc],[x,x+inc]],[[y,y],[y+inc,y+inc]],z+inc, alpha=a)\n",
    "\n",
    "    ax.plot_surface(x,[[y,y],[y+inc,y+inc]],[[z,z+inc],[z,z+inc]], alpha=a)\n",
    "    ax.plot_surface(x+inc,[[y,y],[y+inc,y+inc]],[[z,z+inc],[z,z+inc]], alpha=a)\n",
    "\n",
    "    ax.plot_surface([[x,x],[x+inc,x+inc]],y,[[z,z+inc],[z,z+inc]], alpha=a)\n",
    "    ax.plot_surface([[x,x],[x+inc,x+inc]],y+inc,[[z,z+inc],[z,z+inc]], alpha=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''filter visualization'''\n",
    "def viz1(V):\n",
    "    V = V/np.max(V) * 0.3 \n",
    "    V[V<0]=0\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(V.shape[0]):\n",
    "        for j in range(V.shape[1]):\n",
    "            for k in range(V.shape[2]):\n",
    "                if V[i,j,k]!=0:\n",
    "                    plot_cube(ax,i,j,k,V[i,j,k])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''much faster filter visualization'''\n",
    "# % reset\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "def viz2(V):\n",
    "    V = V/np.max(V) * 0.3 \n",
    "    V[V<0]=0\n",
    "    \n",
    "    x=y=z=t=[]\n",
    "    for i in range(V.shape[0]):\n",
    "        for j in range(V.shape[1]):\n",
    "            for k in range(V.shape[2]):\n",
    "                if V[i,j,k]!=0:\n",
    "                    x = x + [i]\n",
    "                    y = y + [j]\n",
    "                    z = z + [k]\n",
    "                    t = t + [V[i,j,k]]\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    z = np.asarray(z)\n",
    "    t = np.asarray(z)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    im = ax.scatter(x, y, z, c=t, marker='o', s=50)\n",
    "    ax.set_xlabel('X Label')\n",
    "    ax.set_ylabel('Y Label')\n",
    "    ax.set_zlabel('Z Label')\n",
    "    plt.xlim(0,V.shape[0])\n",
    "    plt.ylim(0,V.shape[1])\n",
    "#     plt.zlim((0,V.shape[2]))\n",
    "    cax = fig.add_axes([0.27, 0.8, 0.5, 0.05])\n",
    "    fig.colorbar(im, cax=cax, orientation='horizontal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "\n",
    "# the random data\n",
    "x = np.random.randn(1000)\n",
    "y = np.random.randn(1000)\n",
    "\n",
    "fig = plt.figure(1, figsize=(5.5,5.5))\n",
    "\n",
    "X, Y = meshgrid(x, y)\n",
    "Z1 = bivariate_normal(X, Y, 1.0, 1.0, 0.0, 0.0)\n",
    "Z2 = bivariate_normal(X, Y, 1.5, 0.5, 1, 1)\n",
    "Z = 10 * (Z1 - Z2)\n",
    "\n",
    "title('Nonsense')\n",
    "xlabel('x-stuff')\n",
    "ylabel('y-stuff')\n",
    "\n",
    "# the scatter plot:\n",
    "axScatter = plt.subplot(111)\n",
    "axScatter.scatter(x, y)\n",
    "\n",
    "# set axes range\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 1)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fuel.datasets.hdf5 import H5PYDataset\n",
    "train_set = H5PYDataset('potcup_vox.hdf5', which_sets=('train',))\n",
    "handle = train_set.open()\n",
    "train_data = train_set.get_data(handle, slice(0, 18))\n",
    "train_data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n",
      "[[0, 1, 2, 3], [4, 5, 6, 7]]\n",
      "[[8, 2, 6, 7], [1, 0, 4, 3], [5]]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7]\n",
      "[7, 2, 1, 6, 0, 4, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "% reset\n",
    "from fuel.schemes import (SequentialScheme, ShuffledScheme,\n",
    "                   SequentialExampleScheme, ShuffledExampleScheme)\n",
    "schemes = [SequentialScheme(examples=8, batch_size=4),\n",
    "    ShuffledScheme(examples=9, batch_size=4),\n",
    "    SequentialExampleScheme(examples=8),\n",
    "    ShuffledExampleScheme(examples=8)]\n",
    "for scheme in schemes:\n",
    "    print(list(scheme.get_request_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 1, 5, 5, 5)\n",
      "(24, 16, 10, 10, 10)\n",
      "(24, 1, 6, 6, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "import tarfile\n",
    "tarball = tarfile.open('3DLeNet.pkl', 'r')\n",
    "ps = numpy.load(tarball.extractfile(tarball.getmember('_parameters')))\n",
    "sorted(ps.keys())\n",
    "\n",
    "W0 = ps['|lenet|convolutionalsequence3|conv_0.W']\n",
    "W1 = ps['|lenet|convolutionalsequence3|conv_1.W']\n",
    "W1 = upsample(W1, 2)\n",
    "\n",
    "W1_layer0 = deconv(W0,W1)\n",
    "# V = W1_layer0[10,0,:,:,:]\n",
    "# viz2(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mlp/linear_1.W', '/mlp/linear_1.b', '/mlp/linear_0.b', '/mlp/linear_0.W']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(parameters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tarfile.TarFile at 0x7fc4f1457e90>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''save and load blocks model'''\n",
    "from theano import tensor\n",
    "from blocks.main_loop import MainLoop\n",
    "from blocks.bricks import MLP, Tanh, Softmax\n",
    "from blocks.model import Model\n",
    "mlp = MLP([Tanh(), None], [784, 10, 10])\n",
    "x = tensor.matrix('features')\n",
    "y = tensor.lmatrix('targets')\n",
    "cost = Softmax().categorical_cross_entropy(\n",
    "            y.flatten(), mlp.apply(tensor.flatten(x, outdim=2)))\n",
    "main_loop = MainLoop(None, None, model=Model(cost))\n",
    "from blocks.serialization import dump, load\n",
    "import tarfile\n",
    "with open('main_loop.tar', 'wb') as dst:\n",
    "     dump(main_loop, dst)\n",
    "tarball = tarfile.open('main_loop.tar', 'r')\n",
    "tarball # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_pkl']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tarball.getnames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tarball.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_parameters', '_pkl']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('main_loop.tar', 'wb') as dst:\n",
    "     dump(main_loop, dst,\n",
    "          parameters=main_loop.model.parameters)\n",
    "tarball = tarfile.open('main_loop.tar', 'r')\n",
    "tarball.getnames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['|mlp|linear_0.W', '|mlp|linear_0.b', '|mlp|linear_1.W', '|mlp|linear_1.b']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "ps = numpy.load(tarball.extractfile(tarball.getmember('_parameters')))\n",
    "sorted(ps.keys()) # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<blocks.main_loop.MainLoop at 0x7fc4f131b650>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.close()\n",
    "with open('main_loop.tar', 'rb') as src:\n",
    "     main_loop_loaded = load(src)\n",
    "main_loop_loaded # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/mlp/linear_0.W', '/mlp/linear_0.b', '/mlp/linear_1.W', '/mlp/linear_1.b']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from blocks.serialization import load_parameters\n",
    "with open('main_loop.tar', 'rb') as src:\n",
    "     parameters = load_parameters(src)\n",
    "sorted(parameters.keys()) # doctest: +ELLIPSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95996732026143794"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Adaboost'''\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "clf = AdaBoostClassifier(n_estimators=15)\n",
    "scores = cross_val_score(clf, iris.data, iris.target)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones(3)\n",
    "(a-1).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
